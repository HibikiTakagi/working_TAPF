Timer unit: 1e-06 s

Total time: 12.713 s
File: savetraining.py
Function: main at line 100

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   100                                               @profile
   101                                               def main(self):
   102                                                   #logging.basicConfig(level=logging.DEBUG)
   103                                           
   104                                                   #graph, robots = example_data()
   105         1          0.5      0.5      0.0          graph = GRAPH
   106         1          0.2      0.2      0.0          robots = self.robots
   107         1          9.3      9.3      0.0          with open(self.tmpembeddings_file, "rb") as f:
   108         1        338.1    338.1      0.0              node_embeddings = pickle.load(f)
   109         1         95.1     95.1      0.0          env = RobotsRoutingEnvironment(graph, robots, orirobots = ORIROBOTS, embedding=node_embeddings)
   110         1        224.4    224.4      0.0          encoder = StateEncoder(env, node_embeddings)
   111         1        244.1    244.1      0.0          decoder = ActionDecoder(env)
   112         1       8882.1   8882.1      0.1          policy = DeepQPolicy(state_encoder=encoder, action_decoder=decoder)
   113                                           
   114         1          0.2      0.2      0.0          if PRIMODE:
   115                                                       replay_buffer = PriorityReplayBuffer(capacity=BUF_CAPASITY)
   116                                                       TD_memory = Memory_TDerror(capacity=BUF_CAPASITY, encoder=encoder, decoder=decoder)
   117                                                   else:
   118         1          2.6      2.6      0.0              replay_buffer = ReplayBuffer(capacity=BUF_CAPASITY)
   119                                           
   120         1         65.9     65.9      0.0          with open(self.model_file, "w") as f:
   121         1          0.2      0.2      0.0              if ACMODE:
   122                                                           f.write(f'{policy._actor_net}')
   123                                                           f.write(f'{policy._critic_net}')
   124                                                       else:
   125         1         33.9     33.9      0.0                  f.write(f'{policy._q_network}')
   126                                           
   127         1         25.8     25.8      0.0          self.logfileinitial()
   128         1          0.1      0.1      0.0          target_returns = []
   129         1          0.1      0.1      0.0          returns = []
   130                                                       
   131         1          0.3      0.3      0.0          best_return = INI_BEST_RETURN
   132         1          0.1      0.1      0.0          sync_count = 0
   133         1          0.2      0.2      0.0          losses = []
   134                                           
   135                                                   # start training
   136         1          1.0      1.0      0.0          gamma = env.gamma
   137         1          0.1      0.1      0.0          logt = 0
   138         1          0.1      0.1      0.0          num_of_train = 0
   139                                                   # main training loop
   140         5          1.9      0.4      0.0          for eps in range(TRAIN_EPS):
   141                                           
   142         5          1.1      0.2      0.0              if RANDOMMODE:
   143                                                           env.robot_reset()
   144                                                       
   145         5          0.7      0.1      0.0              if TASKRANDOMMODE and TASKRANDOMSTART<=eps and eps%TASKRANDOMFREQ==0:
   146                                                           env.task_reset()
   147                                                       
   148         5        275.3     55.1      0.0              env.reset()
   149                                                       #logging.info(f"Episode: {eps+1}")
   150                                           
   151                                                       # start = time.perf_counter()
   152         5          2.2      0.4      0.0              eps_return = 0
   153      2505        313.3      0.1      0.0              while (
   154      2505       7258.5      2.9      0.1                  not env.current_state.is_terminal
   155      2500       1482.1      0.6      0.0                  and env.timestep < MAX_TIMESTEP
   156                                                       ):
   157      2500        643.6      0.3      0.0                  s = env.current_state
   158      2500    1163180.3    465.3      9.1                  encoded_s = encoder.encode(s)               #18.5%
   159      2500       3020.7      1.2      0.0                  policy.set_encoded_s(encoded_s=encoded_s)
   160      2500        485.0      0.2      0.0                  if ACMODE:
   161                                                               a, prob = policy.epsilon_greedy(s)
   162                                                           else:
   163      2500    1199233.7    479.7      9.4                      a = policy.epsilon_greedy(s)            # 5.2%
   164      2500     536661.5    214.7      4.2                  rw, new_s = env.transition(a)               # 8.4%
   165                                                           #a = env.exe_a # add by sugimoto
   166      2500       3310.2      1.3      0.0                  eps_return += (gamma**env.timestep) * rw
   167      2500       4960.9      2.0      0.0                  done = env.current_state.is_terminal
   168                                                           #logging.debug(f"T={t+1}: {a} -> {rw}")
   169      2500        550.7      0.2      0.0                  sync_count += 1
   170                                                           
   171      2500        569.5      0.2      0.0                  if ACMODE:
   172                                                               pass
   173      2500        502.3      0.2      0.0                  elif PRIMODE:
   174                                                               #replay_buffer.add((s, a, rw, new_s, done))
   175                                                               encoded_s = encoded_s[0]
   176                                                               encoded_new_s = encoder.encode(new_s)[0]
   177                                                               replay_buffer.add((s, a, rw, new_s, done, encoded_s, encoded_new_s))
   178                                                               # TDerror = TD_memory.get_TDerror(replay_buffer, gamma, q_network, target_network)
   179                                                               # TD_memory.add(TDerror)
   180                                                               TD_memory.add(0)
   181                                                               if len(replay_buffer) < BATCH_SIZE:
   182                                                                   # not enough experience to train, so
   183                                                                   continue
   184                                                           else:
   185      2500        962.3      0.4      0.0                      encoded_s = encoded_s[0]
   186      2500    1194786.0    477.9      9.4                      encoded_new_s = encoder.encode(new_s)[0] #18,8%
   187      2500       4547.0      1.8      0.0                      replay_buffer.add((s, a, rw, new_s, done, encoded_s, encoded_new_s))
   188                                                               #replay_buffer.add((s, a, rw, new_s, done))
   189      2437       2146.2      0.9      0.0                      if len(replay_buffer) < BATCH_SIZE:
   190                                                                   # not enough experience to train, so
   191        63         11.6      0.2      0.0                          continue
   192                                           
   193      2437        370.0      0.2      0.0                  if ACMODE:
   194                                                               policy.ac_train(batch=(s,a,rw,new_s,done),prob=prob,gamma=gamma)
   195      1949       1118.3      0.6      0.0                  elif env.timestep % TRAIN_FREQ == 0:
   196       488         75.7      0.2      0.0                      if PRIMODE:
   197                                                                   if num_of_train < MAX_TRAIN_EPS/2:
   198                                                                       policy.train_batch_without_p(batch=replay_buffer.sample(BATCH_SIZE), gamma=gamma)
   199                                                                       losses.append([eps, policy.loss])
   200                                                                   else:
   201                                                                       policy.train_batch_with_p(batch=replay_buffer, batch_size=BATCH_SIZE, gamma=gamma, TD_memory=TD_memory)
   202                                                                       losses.append([eps, policy.loss])
   203                                                               else:
   204       488    4962159.6  10168.4     39.0                          policy.train_batch(batch=replay_buffer.sample(BATCH_SIZE), gamma=gamma) #37.6%
   205       488        546.8      1.1      0.0                          losses.append([eps, policy.loss])
   206                                                       else:
   207                                                           #logging.info( f"Episode {eps+1}, maximum timestep {t+1}: Return: {eps_return}")
   208         5          1.7      0.3      0.0                  returns.append(eps_return)
   209         5          2.2      0.4      0.0                  logt = env.timestep
   210         5          2.1      0.4      0.0                  if PRIMODE:
   211                                                               if num_of_train + 100 < MAX_TRAIN_EPS/2:
   212                                                                   pass
   213                                                               else:
   214                                                                   TD_memory.update_TDerror(replay_buffer, gamma, policy._q_network, policy._target_network)
   215         5          1.1      0.2      0.0                  if sync_count >= SYNC_FREQ:
   216                                                               # synchronize weight from behavior net to target net
   217         5       1879.7    375.9      0.0                      policy.targetsync()
   218         5          1.5      0.3      0.0                      sync_count = 0
   219                                           
   220                                                       # time_elapsed = time.perf_counter() - start
   221                                                       #logging.info(f"Episode {eps+1}: {time_elapsed:.2f}s elapsed")
   222                                           
   223                                                       #logging.info(f"Episode {eps+1}: Evaluating target policy")
   224         5          0.9      0.2      0.0              if sync_count == 0 or eps==0:
   225         5        234.4     46.9      0.0                  env.reset()
   226         5          1.0      0.2      0.0                  rt = 0
   227      2505        344.2      0.1      0.0                  while (
   228      2505       4896.8      2.0      0.0                      not env.current_state.is_terminal
   229      2500       1230.6      0.5      0.0                      and env.timestep < MAX_TIMESTEP_TARGET
   230                                                           ):
   231      2500       1575.4      0.6      0.0                      s = env.current_state
   232      2500    1210194.7    484.1      9.5                      encoded_s = encoder.encode(s) #4.1%
   233      2500       2709.2      1.1      0.0                      policy.set_encoded_s(encoded_s=encoded_s)
   234      2500        593.7      0.2      0.0                      if ACMODE:
   235                                                                   a, prob = policy.target_greedy(s)
   236                                                               else:
   237      2500    1419894.8    568.0     11.2                          a = policy.target_greedy(s)
   238      2500     510665.9    204.3      4.0                      rw, new_s = env.transition(a)
   239      2500       3250.2      1.3      0.0                      rt += (gamma**env.timestep) * rw
   240         4          1.3      0.3      0.0                  if rt > best_return:
   241         4          0.5      0.1      0.0                      best_return = rt
   242                                                               #best_network.set_weights(target_network.get_weights())
   243         4       1517.2    379.3      0.0                      policy.bestsync()
   244         5          3.1      0.6      0.0                  envt = env.timestep
   245         5          2.3      0.5      0.0                  envcc = env.collision_count
   246                                           
   247         4          2.3      0.6      0.0              if num_of_train%100==0:
   248         1      12921.6  12921.6      0.1                  fig_convergence_return, ax = plt.subplots()
   249         1        454.1    454.1      0.0                  plt.plot(returns,label="train_return") 
   250         1        394.5    394.5      0.0                  plt.plot(target_returns,label="target_return")
   251         1         40.3     40.3      0.0                  ax.set_xlabel('episodes')
   252         1         33.7     33.7      0.0                  ax.set_ylabel('Returns')
   253         1        280.1    280.1      0.0                  ax.grid(True)
   254         1       1676.6   1676.6      0.0                  plt.legend(loc=0)
   255         1      47810.7  47810.7      0.4                  fig_convergence_return.savefig(self.dirnamesla+"convergence.png")
   256         1      18725.7  18725.7      0.1                  plt.clf()
   257         1         18.2     18.2      0.0                  plt.close()
   258                                           
   259         1      10513.5  10513.5      0.1                  fig_loss, ax = plt.subplots()
   260                                                           #plt.plot(losses, label='loss')
   261         1          6.6      6.6      0.0                  x_eps, y_loss = zip(*losses)
   262                                                           #plt.scatter(x_eps,y_loss, label='loss')
   263         1        486.0    486.0      0.0                  plt.plot(x_eps,y_loss, '-',label='loss')
   264         1         42.6     42.6      0.0                  ax.set_xlabel('episodes')
   265         1         32.0     32.0      0.0                  ax.set_ylabel('loss')
   266         1        293.1    293.1      0.0                  ax.grid(True)
   267         1        985.8    985.8      0.0                  plt.legend(loc=0)
   268         1      47310.6  47310.6      0.4                  fig_loss.savefig(self.dirnamesla+"loss.png")
   269         1      18727.5  18727.5      0.1                  plt.clf()
   270         1         17.5     17.5      0.0                  plt.close()
   271                                           
   272         1      17649.6  17649.6      0.1                  policy.save_network(self.tmptrain_file,self.tmptarget_file,self.tmpbest_file)
   273                                           
   274                                                           
   275         5          2.4      0.5      0.0              target_returns.append(rt)
   276                                                       #logging.info(f"Episode {eps+1}: Evaluating: tooks {env.timestep} steps with return {rt}")
   277         5        406.4     81.3      0.0              self.logfile(num_of_train, logt, eps_return, a, policy._epsilon, envt, rt, envcc)
   278         5         11.8      2.4      0.0              policy.setepsilon()
   279         5          3.0      0.6      0.0              num_of_train += 1
   280                                           
   281         1        157.8    157.8      0.0          np.save(self.tmpreturns, returns)
   282         1         78.8     78.8      0.0          np.save(self.tmptargetreturns, target_returns)
   283         1        226.0    226.0      0.0          np.save(self.tmplosses, losses)
   284         1      62820.4  62820.4      0.5          policy.save_network(self.tmptrain_file,self.tmptarget_file,self.tmpbest_file)
   285         1        105.3    105.3      0.0          self.save_bestreturn(best_return)
   286         1         28.1     28.1      0.0          self.save_synccount(sync_count)
   287         1       4207.5   4207.5      0.0          with open(self.tmpreplaybuffer_file, "wb") as f:
   288         1      39971.4  39971.4      0.3              pickle.dump(replay_buffer, f, protocol=pickle.HIGHEST_PROTOCOL)
   289         1          1.8      1.8      0.0          if PRIMODE:
   290                                                       with open(self.tmpreplayerrorbuffer_file, "wb") as f:
   291                                                           pickle.dump(TD_memory, f, protocol=pickle.HIGHEST_PROTOCOL)
   292                                           
   293         1      10607.5  10607.5      0.1          fig_convergence_return, ax = plt.subplots()
   294         1        425.3    425.3      0.0          plt.plot(returns,label="train_return") 
   295         1        398.3    398.3      0.0          plt.plot(target_returns,label="target_return")
   296         1         41.1     41.1      0.0          ax.set_xlabel('episodes')
   297         1         30.8     30.8      0.0          ax.set_ylabel('Returns')
   298         1        323.3    323.3      0.0          ax.grid(True)
   299         1       1431.1   1431.1      0.0          plt.legend(loc=0)
   300         1      53988.9  53988.9      0.4          fig_convergence_return.savefig(self.dirnamesla+"convergence.png")
   301         1      18659.3  18659.3      0.1          plt.clf()
   302         1         16.1     16.1      0.0          plt.close()
   303                                                   #'''
   304         1      10336.1  10336.1      0.1          fig_loss, ax = plt.subplots()
   305                                                   #plt.plot(losses, label='loss')
   306         1         68.9     68.9      0.0          x_eps, y_loss = zip(*losses)
   307                                                   #plt.scatter(x_eps,y_loss, label='loss')
   308         1        455.6    455.6      0.0          plt.plot(x_eps,y_loss, '-',label='loss')
   309         1         40.0     40.0      0.0          ax.set_xlabel('episodes')
   310         1         30.0     30.0      0.0          ax.set_ylabel('loss')
   311         1        277.8    277.8      0.0          ax.grid(True)
   312         1        995.7    995.7      0.0          plt.legend(loc=0)
   313         1      51619.7  51619.7      0.4          fig_loss.savefig(self.dirnamesla+"loss.png")
   314         1      18716.0  18716.0      0.1          plt.clf()
   315         1         17.3     17.3      0.0          plt.close()
   316                                                   
   317                                                   #'''

