Timer unit: 1e-06 s

Total time: 46.9644 s
File: /working/AGV_20240115/simulation/QNet.py
Function: make_localedge at line 495

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   495                                               @profile
   496                                               def make_localedge(self, nodes_device, connect):
   497    233408    4634119.0     19.9      9.9          nodes = nodes_device.to('cpu').numpy()
   498    233408   36379684.7    155.9     77.5          edges_index = self.make_localedge_numpy(nodes, connect)
   499    233408    5908387.4     25.3     12.6          edges_torch = torch.tensor(edges_index, dtype=torch.int64).to(DEVICE) # for Graph SAGE int64 (not int32)
   500    233408      42241.7      0.2      0.1          return edges_torch

Total time: 27.6168 s
File: /working/AGV_20240115/simulation/QNet.py
Function: make_localedge_numpy at line 502

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   502                                               @profile
   503                                               def make_localedge_numpy(self, nodes_numpy, connect):
   504    233408    1812995.6      7.8      6.6          node_to_idx = {node: idx for idx, node in enumerate(nodes_numpy)} # ここのせいで、最終的に抜き出しのindexが-1になっている注意
   505    233408     579821.0      2.5      2.1          nodes_set = set(nodes_numpy)  # Convert to a set for faster lookup
   506    233408      45178.5      0.2      0.2          edges_from = []
   507    233408      39136.8      0.2      0.1          edges_to = []
   508                                           
   509   4901568    1061532.0      0.2      3.8          for start_node in nodes_numpy:
   510   4668160     974957.7      0.2      3.5              start_idx = node_to_idx[start_node]
   511  11329128    8371493.7      0.7     30.3              for dest_node in connect[start_node]:
   512   6660968    6320907.8      0.9     22.9                  if dest_node in nodes_set:        
   513   4641310    5748789.8      1.2     20.8                      dest_idx = node_to_idx[dest_node]
   514   4641310    1310049.9      0.3      4.7                      edges_from.append(start_idx)
   515   4641310    1287839.3      0.3      4.7                      edges_to.append(dest_idx)
   516                                           
   517    233408      64122.3      0.3      0.2          return [edges_from, edges_to]

Total time: 55.7134 s
File: /working/AGV_20240115/simulation/QNet.py
Function: make_gnn_dataset at line 519

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   519                                               @profile
   520                                               def make_gnn_dataset(self, nodes_info, nodes_float, connect):
   521    233408   47959747.6    205.5     86.1          edges_index = self.make_localedge(nodes_float, connect)
   522    233408    7753679.5     33.2     13.9          return Data(x=nodes_info, edge_index=edges_index)  

Total time: 107.323 s
File: /working/AGV_20240115/simulation/QNet.py
Function: forward at line 533

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   533                                               @profile
   534                                               def forward(self, x):
   535                                                   #### Graph #####
   536                                                   #print(x.shape)
   537      1981       4297.7      2.2      0.0          batch_num = x.shape[0]
   538      1981      22070.7     11.1      0.0          x_reshape = torch.reshape(x, (batch_num, KNN_AGENTS, -1))
   539                                                   #print(x_reshape.shape)
   540      1981      43651.3     22.0      0.0          x_split = torch.split(x_reshape, (self.startnode_dims,self.destnode_dims,self.fromnodes,self.tonodes, self.elsedim), dim=2)
   541      1981      11228.0      5.7      0.0          x_start_infos = torch.reshape(x_split[0], (batch_num, KNN_AGENTS, self.fromnodes, -1))
   542      1981       8302.3      4.2      0.0          x_dest_infos = torch.reshape(x_split[1], (batch_num, KNN_AGENTS, self.tonodes, -1))
   543      1981      81102.4     40.9      0.1          x_start_graphs = x_split[2].clone().detach().to(torch.int32)
   544      1981      37959.6     19.2      0.0          x_dest_graphs = x_split[3].clone().detach().to(torch.int32)
   545      1981        552.4      0.3      0.0          x_elses = x_split[4]
   546                                           
   547      1981        709.5      0.4      0.0          x_feat = []
   548      1981     152258.1     76.9      0.1          x_start_nodes_zero = x_start_graphs[:,:,0].to('cpu').numpy()
   549      1981      84482.9     42.6      0.1          x_dest_nodes_zero = x_dest_graphs[:,:,0].to('cpu').numpy()
   550                                           
   551                                                   #print(x_start_infos.shape)
   552      1981        621.8      0.3      0.0          datasets = []
   553     60333      20971.6      0.3      0.0          for batch in range(batch_num):
   554     58352     325412.5      5.6      0.3              x_start_info = x_start_infos[batch]
   555     58352     132582.2      2.3      0.1              x_dest_info = x_dest_infos[batch]
   556     58352     118444.8      2.0      0.1              x_start_graph = x_start_graphs[batch]
   557     58352     113443.1      1.9      0.1              x_dest_graph = x_dest_graphs[batch]
   558    291760     111542.6      0.4      0.1              for robot in range(KNN_AGENTS):
   559    233408    1020358.2      4.4      1.0                  x_start_info_robot = x_start_info[robot]
   560    233408     475539.2      2.0      0.4                  x_dest_info_robot = x_dest_info[robot]
   561    233408     466595.3      2.0      0.4                  x_start_graph_robot = x_start_graph[robot]
   562    233408     455456.9      2.0      0.4                  x_dest_graph_robot = x_dest_graph[robot]
   563                                           
   564                                                           #print(x_start_info_robot.shape)
   565                                                           #print(x_start_graph_robot.shape)
   566    233408    2946028.4     12.6      2.7                  x_info = torch.cat([x_start_info_robot, x_dest_info_robot], dim=0)
   567    233408    1906694.9      8.2      1.8                  x_graph = torch.cat([x_start_graph_robot, x_dest_graph_robot], dim=0)
   568                                                           #print(x_info.shape)
   569                                                           #print(x_graph.shape)
   570                                           
   571    233408   56597729.6    242.5     52.7                  datasets.append(self.make_gnn_dataset(x_info, x_graph, CONNECTDICT))
   572      1981      89717.8     45.3      0.1          dataset_loader = DataLoader(datasets, batch_size=len(datasets), shuffle=False)
   573      1981        513.5      0.3      0.0          x_infos = []
   574      3962   12519052.8   3159.8     11.7          for data in dataset_loader:
   575                                                       #print(data.x.shape)
   576                                                       #print(data.edge_index)
   577      1981    1307032.8    659.8      1.2              x_data = self.state_graph_l1(data.x, data.edge_index)
   578      1981      29090.4     14.7      0.0              x_data = F.relu(x_data)
   579      1981       1527.7      0.8      0.0              x_infos.append(x_data)
   580                                                   
   581      1981      55168.7     27.8      0.1          x_infos_after_gnn = torch.cat(x_infos)
   582      1981      21109.0     10.7      0.0          x_infos_after_gnn = x_infos_after_gnn.reshape((batch_num, KNN_AGENTS, self.fromnodes+self.tonodes, -1))
   583                                           
   584     60300      15141.0      0.3      0.0          for batch in range(batch_num):
   585     58320     271575.3      4.7      0.3              x_info = x_infos_after_gnn[batch]
   586     58320     126387.7      2.2      0.1              x_start_graph = x_start_graphs[batch]
   587     58320     119575.8      2.1      0.1              x_dest_graph = x_dest_graphs[batch]
   588     58320     130574.4      2.2      0.1              x_else = x_elses[batch]
   589                                           
   590     58320      51104.2      0.9      0.0              x_start_node_zero = x_start_nodes_zero[batch]
   591     58320      18840.9      0.3      0.0              x_dest_node_zero = x_dest_nodes_zero[batch]
   592                                           
   593     58320      11287.4      0.2      0.0              x_tmp_feat = []
   594    291597     114654.0      0.4      0.1              for robot in range(KNN_AGENTS):
   595    233278     948707.0      4.1      0.9                  x_info_robot = x_info[robot]
   596    233278     486396.4      2.1      0.5                  x_start_graph_robot = x_start_graph[robot]
   597    233278     457720.3      2.0      0.4                  x_dest_graph_robot = x_dest_graph[robot]
   598    233278     455311.3      2.0      0.4                  x_else_robot = x_else[robot]
   599    233278     158385.1      0.7      0.1                  x_start_node_zero_robot = x_start_node_zero[robot]
   600    233278      58336.8      0.3      0.1                  x_dest_node_zero_robot = x_dest_node_zero[robot]
   601                                           
   602    233278    2113993.5      9.1      2.0                  x_graph_robot = torch.cat([x_start_graph_robot, x_dest_graph_robot], dim=0)
   603                                           
   604    233278     191290.8      0.8      0.2                  abst_list = ACTION_LIST[x_start_node_zero_robot][:]
   605    233278      92774.2      0.4      0.1                  abst_list.append(x_dest_node_zero_robot)
   606                                                           #print(abst_list)
   607    233278    8797322.2     37.7      8.2                  rows = self.find_indices(abst_list, x_graph_robot)
   608    233278   11914350.9     51.1     11.1                  x_tmp_feat.append(torch.cat([torch.flatten(x_info_robot[rows]), x_else_robot]))
   609                                           
   610     58319     622044.4     10.7      0.6              x_tmp_feat = torch.cat(x_tmp_feat)
   611     58319      30538.7      0.5      0.0              x_feat.append(x_tmp_feat)
   612                                           
   613      1980      41888.9     21.2      0.0          x = torch.cat(x_feat)
   614      1980      12460.2      6.3      0.0          x = torch.reshape(x, (batch_num, -1))
   615                                           
   616                                                   #### Graph End #####
   617                                           
   618      1980     134665.6     68.0      0.1          x = F.relu(self.l1(x))
   619      1980        759.1      0.4      0.0          if DIM2:
   620                                                       x = F.relu(self.l2(x))
   621                                           
   622      1980        543.3      0.3      0.0          if DUELINGMODE:
   623      1980     372495.9    188.1      0.3              adv = self.ladv_last(x)
   624      1980     327099.9    165.2      0.3              v = self.lv_last(x)
   625      1980      29128.1     14.7      0.0              averagea = adv.mean(1, keepdim=True)
   626      1980      56040.4     28.3      0.1              return v.expand(-1, self.action_dims) + (adv - averagea.expand(-1, self.action_dims))
   627                                                   else:
   628                                                       x = self.l_last(x)
   629                                                       return x

Total time: 127.073 s
File: savetraining.py
Function: main at line 88

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    88                                               @profile
    89                                               def main(self):
    90         1    1740050.3    2e+06      1.4          policy = DeepQPolicy(self.env.net_state_dim, self.env.net_act_dim, self.env.num_agents, self.env.graph)
    91                                                   #replay_buffer = ReplayBuffer()
    92                                           
    93         1          1.3      1.3      0.0          sync_cnt=0
    94         1          0.2      0.2      0.0          target_returns = [] # target return log
    95         1          0.1      0.1      0.0          returns = [] # return log
    96         1          0.2      0.2      0.0          losses = [] # loss log
    97         1          0.2      0.2      0.0          num_of_train = 0
    98                                           
    99         2      13904.2   6952.1      0.0          with open(self.model_file, "w") as f:
   100         1         68.3     68.3      0.0                  f.write(f'{policy._q_network}')
   101                                           
   102                                                   
   103        11       4561.5    414.7      0.0          for eps in tqdm(range(TRAIN_EPS)): # repeat for DQN train episode
   104                                                       # var init
   105        11          3.4      0.3      0.0              log_eps_return = 0
   106        11          1.5      0.1      0.0              done = False
   107        11      70373.2   6397.6      0.1              observation = self.env.reset()
   108        11         15.6      1.4      0.0              observation_next = observation.copy()
   109        11         12.4      1.1      0.0              replay_obs = self.env.replay_obs.copy()
   110        11         10.8      1.0      0.0              replay_obs_next = self.env.replay_obs_next.copy() # obs next間のエージェントが種類が違うせいで性能落ち？
   111                                                       
   112      1030        701.3      0.7      0.0              while not done: # Do task and train in one episode while not terminated or truncated
   113      1020   15679517.7  15372.1     12.3                  action = policy.epsilon_greedy(observation) # get next action from policy
   114      1020    2575644.0   2525.1      2.0                  observation_next, reward, terminated, truncated, _ = self.env.step(action) # do action
   115      1020        518.7      0.5      0.0                  done = terminated or truncated
   116      1020        364.1      0.4      0.0                  if KNN_MODE: # What is KNN mode?
   117      1020       1424.7      1.4      0.0                      replay_obs_next[:] = self.env.replay_obs_next[:]
   118                                           
   119      1020       1910.5      1.9      0.0                  log_eps_return += (GAMMA**self.env.num_steps)*reward # reward decreases as step increases
   120      1020        311.5      0.3      0.0                  sync_cnt += 1
   121                                                           # record replay
   122      1020        199.9      0.2      0.0                  if KNN_MODE:
   123      1020     322768.6    316.4      0.3                      self.replay_buffer.add((replay_obs.copy(), action.copy(), reward, replay_obs_next.copy(), terminated))
   124                                                           else:
   125                                                               self.replay_buffer.add((observation.copy(), action.copy(), reward, observation_next.copy(), terminated))
   126      1020       1751.2      1.7      0.0                  observation[:] = observation_next[:] # get updated ovservation(env's situation?)
   127      1020        313.6      0.3      0.0                  if KNN_MODE:
   128      1020       1190.4      1.2      0.0                      replay_obs[:] = self.env.replay_obs[:]
   129      1020       1922.1      1.9      0.0                  if len(self.replay_buffer) < BATCH_SIZE:
   130        63         18.8      0.3      0.0                      continue
   131                                           
   132       957        561.2      0.6      0.0                  if self.env.num_steps % TRAIN_FREQ == 0:
   133       190  100671432.2 529849.6     79.2                      policy.train_batch(batch=self.replay_buffer.sample(BATCH_SIZE)) # TRAIN from replay
   134                                                       
   135                                                       else:
   136        10          7.6      0.8      0.0                  returns.append(log_eps_return)
   137        10          3.6      0.4      0.0                  if sync_cnt>=SYNC_FREQ:
   138         2       1809.5    904.7      0.0                      policy.sync() # update(sync) policy from trained data when enough steps have ended
   139         2          0.8      0.4      0.0                      sync_cnt=0
   140                                                       
   141        10          8.0      0.8      0.0              losses.append([eps, policy.loss])
   142                                           
   143        10          5.1      0.5      0.0              if sync_cnt==0 or eps==0: # if policy has synchronised or first episode
   144         3          0.8      0.3      0.0                  log_target_return = 0
   145         3          0.6      0.2      0.0                  done = False
   146         3      18649.6   6216.5      0.0                  observation = self.env.reset()
   147       306         84.1      0.3      0.0                  while not done:
   148       303    4995307.6  16486.2      3.9                      action = policy.target_greedy(observation)
   149       303     753267.5   2486.0      0.6                      observation_next, reward, terminated, truncated, _ = self.env.step(action) 
   150       303        159.4      0.5      0.0                      done = terminated or truncated
   151       303        636.4      2.1      0.0                      log_target_return += (GAMMA**self.env.num_steps)*reward
   152       303        119.1      0.4      0.0                      observation = observation_next
   153                                                       
   154        10          4.0      0.4      0.0              if num_of_train%100==0:
   155                                                           # save return and losses log every 100 loops
   156         1     115112.6 115112.6      0.1                  self.save_fig_conv_return(returns=returns, target_returns=target_returns)
   157         1      88642.1  88642.1      0.1                  self.save_fig_losses(losses=losses)
   158                                           
   159         1       9608.1   9608.1      0.0                  policy.save_network(self.tmptrain_file,self.tmptarget_file)
   160                                           
   161        10          7.5      0.7      0.0              target_returns.append(log_target_return)
   162        10         61.0      6.1      0.0              policy.setepsilon()
   163        10          5.6      0.6      0.0              num_of_train += 1
   164                                           
   165                                                   np.save(self.tmpreturns, returns)
   166                                                   np.save(self.tmptargetreturns, target_returns)
   167                                                   np.save(self.tmplosses, losses)
   168                                                   policy.save_network(self.tmptrain_file, self.tmptarget_file)
   169                                           
   170                                                   self.save_fig_conv_return(returns=returns, target_returns=target_returns)
   171                                                   self.save_fig_losses(losses=losses)
   172                                           
   173                                                   writer = SummaryWriter(self.boarddir)
   174                                                   for i in range(len(target_returns)):
   175                                                       writer.add_scalar("Returns", returns[i], i)
   176                                                       writer.add_scalar("Target Returns", target_returns[i], i)
   177                                                       #writer.add_scalar("loss", losses[i], i)
   178                                                   writer.close()

