{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym[classic_control]\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'classic-control'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (3.0.0)\n",
      "Collecting gym-notices>=0.0.4 (from gym[classic_control])\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting pygame==2.1.0 (from gym[classic_control])\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827624 sha256=6e62046b7ca20e2ce2de1a5bc257f3a809bc85760ffbaf094aa39d7bb4bbdf75\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, pygame, gym\n",
      "  Attempting uninstall: pygame\n",
      "    Found existing installation: pygame 2.5.2\n",
      "    Uninstalling pygame-2.5.2:\n",
      "      Successfully uninstalled pygame-2.5.2\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8 pygame-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as pt\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "GAMMA = 0.9999 # 0.99\n",
    "LR = 0.0025 # 0.01\n",
    "EPISODES_TO_TRAIN = 15 # 10\n",
    "HIDDEN_SIZE = 256 # 128\n",
    "BELLMAN_STEPS = 4\n",
    "BETA_ACTOR, BETA_CRITIC, BETA_ENTROPY = 1, 0.001, 1, \n",
    "\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=[\n",
    "                            'state',\n",
    "                            'action',\n",
    "                            'value',\n",
    "                            'log_prob',\n",
    "                            'entropy',\n",
    "                            'reward',\n",
    "                            'next_state',\n",
    "                            'next_value',   \n",
    "                            'is_done'\n",
    "                            ])\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, inputs_features, n_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(inputs_features, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(inputs_features, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        probs = self.actor(x)\n",
    "        distr  = dist.Categorical(probs)\n",
    "        return distr, value\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, disc_factor, net, optimizer, scheduler):\n",
    "        self.env = env\n",
    "        self.first_state = env.reset\n",
    "        self.disc_factor = disc_factor\n",
    "        self.net = net\n",
    "        self.scheduler = scheduler\n",
    "        self.optimizer = optimizer\n",
    "        self.distr = ''\n",
    "\n",
    "    def explore(self):\n",
    "        episodes = []\n",
    "        state, _ = self.first_state()\n",
    "        state = pt.from_numpy(state).float()\n",
    "        while True:\n",
    "            # SA...\n",
    "            distr, value = self.net.forward(state) # value with grads\n",
    "            action = distr.sample()\n",
    "            log_prob = distr.log_prob(action) # with grads\n",
    "            #print(distr.entropy())\n",
    "            entropy = distr.entropy().mean() # with grads\n",
    "            \n",
    "            # RS'...\n",
    "            next_state, reward, terminated, truncated , _ = self.env.step(action.item())\n",
    "            is_done = truncated\n",
    "            # V\n",
    "            next_state = pt.from_numpy(next_state).float()\n",
    "            _, next_value = self.net.forward(next_state) # next_value with grads\n",
    "            \n",
    "            # store results\n",
    "            episodes.append(EpisodeStep(state=state,\n",
    "                                        action=action,\n",
    "                                        value=value,\n",
    "                                        log_prob=log_prob,\n",
    "                                        entropy=entropy,\n",
    "                                        reward=reward,\n",
    "                                        next_state=next_state,\n",
    "                                        next_value=next_value,\n",
    "                                        is_done=is_done))\n",
    "            if is_done:\n",
    "                yield episodes\n",
    "                self.distr = distr\n",
    "                episodes = []\n",
    "                state, _ = self.first_state()\n",
    "                state = pt.from_numpy(state).float()\n",
    "            state = next_state\n",
    "\n",
    "    def calc_qvals(self, rewards):\n",
    "        w = []\n",
    "        sum_r = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            sum_r *= self.disc_factor\n",
    "            sum_r += reward\n",
    "            w.append(sum_r)\n",
    "        w = list(reversed(w))\n",
    "        w = pt.tensor(w)\n",
    "        # normalize\n",
    "        w = w - w.quantile(0.5)\n",
    "        # w = (w - w.mean()) / w.std()\n",
    "        # w = w.tolist()\n",
    "        return w\n",
    "    \n",
    "    def calc_qvals_plus_v2(self, rewards, next_values, bellman_steps):\n",
    "        result = []\n",
    "        steps = [bellman_steps] * len(rewards)\n",
    "        for i in range(len(rewards)):\n",
    "            # ...4 4 4 4 4 3 2 1\n",
    "            if i + steps[i] > len(rewards):\n",
    "                take_step = i + steps[i] - len(rewards)\n",
    "                steps[i] = steps[i] - take_step\n",
    "        for step, rew in zip(steps, range(len(rewards))):\n",
    "            # Q = sum_i_n(gamma^n*r_i) + gamma^n*V using just belllman steps\n",
    "            if step >= bellman_steps:\n",
    "                q = self.calc_qvals(rewards[rew: rew + step])\n",
    "                q = q[0]\n",
    "                q_v = q + GAMMA ** bellman_steps * next_values[rew + step - 1]\n",
    "                result.append(q_v)\n",
    "            else:\n",
    "                q = self.calc_qvals(rewards[rew: rew + step])[0]\n",
    "                result.append(q)\n",
    "        return result\n",
    "\n",
    "    def learn(self):\n",
    "        global EPISODES_TO_TRAIN\n",
    "        states, actions, rewards, values, next_states, next_values = [], [], [], [], [], []\n",
    "        entropys, log_probs, qvals, accum_rewards, q_plus_vss, advs = [], [], [], [], [], []\n",
    "        sum_rewards, base_qvals, scales = 0.0, 0.0, 0.0\n",
    "        episodes, loops, best_episodes = 0, 0, 0\n",
    "        \n",
    "        for episode, explored in enumerate(self.explore()):\n",
    "            #print(\"episode\")\n",
    "            #print(episode)\n",
    "            # EXPLORE\n",
    "            states.extend([x.state for x in explored])\n",
    "            actions.extend([x.action for x in explored])\n",
    "            log_probs.extend([x.log_prob for x in explored])\n",
    "            values.extend([x.value for x in explored])\n",
    "            values_copy = values.copy()\n",
    "            entropys.extend([x.entropy for x in explored])\n",
    "            rewards.extend([x.reward for x in explored])\n",
    "            next_states.extend([x.next_state for x in explored])\n",
    "            next_values.extend([x.next_value for x in explored])\n",
    "            next_values_copy = next_values.copy()\n",
    "            # steps.extend([[obs.step for obs in explored][-1]]) # just # steps at the last episode\n",
    "\n",
    "            qvals.extend(self.calc_qvals(rewards))\n",
    "            # advs.extend(adv)\n",
    "            #print(\"values\")\n",
    "            #print(self.calc_qvals(rewards).shape)\n",
    "            sum_rewards = sum(rewards)\n",
    "            rewards.clear()\n",
    "            next_values.clear()\n",
    "            values.clear()\n",
    "            # q_plus_vs, adv = 0, 0\n",
    "            episodes += 1\n",
    "            \n",
    "\n",
    "            # CHECK Solution\n",
    "            accum_rewards.append(sum_rewards)\n",
    "            mean_100th_rew = np.mean(accum_rewards[-100:])\n",
    "            if loops <= 100 and mean_100th_rew > 110:\n",
    "                print(f\"Solved in {episode} episodes, {loops} loops!\")\n",
    "                break\n",
    "            if episodes < EPISODES_TO_TRAIN:\n",
    "                continue  # continue yielding episodes and...\n",
    "            # ...train net for better action prediction\n",
    "\n",
    "            # Critic Loss\n",
    "            # train_state_state = pt.stack(states)\n",
    "            # distr, value = self.net.forward(train_state_state) # .to('cuda', non_blocking=True)\n",
    "            print(len(qvals))\n",
    "            \n",
    "            print(pt.stack(qvals).shape)\n",
    "            print(pt.stack(values_copy).shape)\n",
    "            ast = pt.stack(qvals)-pt.stack(values_copy)\n",
    "            print(ast.shape)\n",
    "            print(pt.stack(log_probs).shape)\n",
    "            print(values_copy.shape)\n",
    "            critic_loss = 0.5 * pt.mean((pt.stack(qvals) - pt.stack(values_copy))**2) #\n",
    "\n",
    "            # Actor Loss\n",
    "            \n",
    "            actor_loss = pt.stack(log_probs) * pt.stack(qvals) # .to('cuda', non_blocking=True)\n",
    "            actor_loss = pt.mean(actor_loss)\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = pt.mean(self.distr.entropy())\n",
    "\n",
    "            # Full loss\n",
    "            loss = BETA_ACTOR * -actor_loss + BETA_CRITIC * critic_loss + BETA_ENTROPY * -entropy_loss\n",
    "            loss_detail = (BETA_ACTOR * -actor_loss.detach().item(), BETA_CRITIC * critic_loss.detach().item(), BETA_ENTROPY * -entropy_loss.detach().item())\n",
    "\n",
    "            # Backward\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.net.parameters(), 5) # security grad explosion\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step() # reduce lr by x amount\n",
    "\n",
    "            # Batch info (mean): s, a, r, s', q,\n",
    "            print(f'Loop: {loops} | Episode: {episode} | Last 100 Rew: {mean_100th_rew:,.2f} | Loss: {loss:+,.3f} ( {loss_detail[0]:,.2f} {loss_detail[1]:,.2f} {loss_detail[2]:,.2f} ) ')\n",
    "\n",
    "            # Saving model\n",
    "            # crc_tz = timezone(\"America/Costa_Rica\")\n",
    "            # now = datetime.now(tz=crc_tz)\n",
    "            # now = datetime.strftime(now, \"%b_%d_%Y\")\n",
    "            # if episodes < best_episodes:\n",
    "            #     pt.save(self.net.state_dict(), f'cartpole_rl_agent_{now}_episodes_{episodes}.pt')\n",
    "            #     best_episodes = episodes\n",
    "\n",
    "            episodes = 0\n",
    "            EPISODES_TO_TRAIN += 1\n",
    "            states, actions, log_probs, values, entropys = [], [], [], [], []\n",
    "            states, actions, q_plus_vss, advs, next_values, qvals = [], [], [], [], [], []\n",
    "            loops += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "torch.Size([7500])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 7500])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m####################################\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Agent\u001b[39;00m\n\u001b[1;32m     24\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, GAMMA, actor_critic, ac_optimizer, ac_scheduler)\n\u001b[0;32m---> 25\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 198\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m ast \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mstack(qvals)\u001b[38;5;241m-\u001b[39mpt\u001b[38;5;241m.\u001b[39mstack(values_copy)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(ast\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvalues_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m    199\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m pt\u001b[38;5;241m.\u001b[39mmean((pt\u001b[38;5;241m.\u001b[39mstack(qvals) \u001b[38;5;241m-\u001b[39m pt\u001b[38;5;241m.\u001b[39mstack(values_copy))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Actor Loss\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ####################################\n",
    "    # Setups\n",
    "    print('*'*50)\n",
    "\n",
    "    ####################################\n",
    "    # Environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    ####################################\n",
    "    # Neural Networks\n",
    "    actor_critic = ActorCritic(inputs_features=env.observation_space.shape[0],\n",
    "                                n_actions=env.action_space.n,\n",
    "                                hidden_size=HIDDEN_SIZE,\n",
    "                                    )\n",
    "    actor_critic = actor_critic.float()\n",
    "    ac_optimizer = pt.optim.Adam(actor_critic.parameters(), lr=LR) # , lr=LR Adam AdamW Adamax\n",
    "    lmbda = lambda epoch: 0.99\n",
    "    ac_scheduler = pt.optim.lr_scheduler.MultiplicativeLR(ac_optimizer, lr_lambda=lmbda)\n",
    "    \n",
    "    ####################################\n",
    "    # Agent\n",
    "    agent = Agent(env, GAMMA, actor_critic, ac_optimizer, ac_scheduler)\n",
    "    agent.learn()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
